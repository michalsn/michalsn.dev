<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Embeddings on michalsn</title>
    <link>https://michalsn.dev/tags/embeddings/</link>
    <description>Recent content in Embeddings on michalsn</description>
    <generator>Hugo -- 0.127.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Jun 2024 09:44:19 +0100</lastBuildDate>
    <atom:link href="https://michalsn.dev/tags/embeddings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Working with PHP, Ollama and embeddings</title>
      <link>https://michalsn.dev/posts/working-with-php-ollama-and-embeddings/</link>
      <pubDate>Sun, 09 Jun 2024 09:44:19 +0100</pubDate>
      <guid>https://michalsn.dev/posts/working-with-php-ollama-and-embeddings/</guid>
      <description>While LLMs, such as the popular GPT family models, are incredibly advanced, they do have their limitations. Primarily, they rely on a static set of knowledge learned during their training phase, which means they might lack specific knowledge on certain topics.
One of the key concepts in working with textual data is embeddings. These are representations of text in a dense vector space, where similar items are mapped to nearby points.</description>
    </item>
  </channel>
</rss>
