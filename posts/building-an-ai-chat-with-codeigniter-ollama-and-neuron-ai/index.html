<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building an AI Chat with CodeIgniter, Ollama, and Neuron AI | michalsn</title><meta name=keywords content="codeigniter4,php,ai,ollama,neuron-ai,sse,streaming"><meta name=description content="Learn how to build a modern AI-powered chat application with CodeIgniter 4, featuring real-time SSE streaming, persistent conversation history, and function calling capabilities."><meta name=author content><link rel=canonical href=https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://michalsn.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michalsn.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michalsn.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://michalsn.dev/apple-touch-icon.png><link rel=mask-icon href=https://michalsn.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=/css/custom.css><script src=/js/custom.js></script><meta property="og:url" content="https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/"><meta property="og:site_name" content="michalsn"><meta property="og:title" content="Building an AI Chat with CodeIgniter, Ollama, and Neuron AI"><meta property="og:description" content="Learn how to build a modern AI-powered chat application with CodeIgniter 4, featuring real-time SSE streaming, persistent conversation history, and function calling capabilities."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-12T19:55:21+01:00"><meta property="article:modified_time" content="2026-02-12T19:55:21+01:00"><meta property="article:tag" content="Codeigniter4"><meta property="article:tag" content="Php"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Neuron-Ai"><meta property="article:tag" content="Sse"><meta property="og:image" content="https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/og.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/og.png"><meta name=twitter:title content="Building an AI Chat with CodeIgniter, Ollama, and Neuron AI"><meta name=twitter:description content="Learn how to build a modern AI-powered chat application with CodeIgniter 4, featuring real-time SSE streaming, persistent conversation history, and function calling capabilities."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michalsn.dev/posts/"},{"@type":"ListItem","position":2,"name":"Building an AI Chat with CodeIgniter, Ollama, and Neuron AI","item":"https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building an AI Chat with CodeIgniter, Ollama, and Neuron AI","name":"Building an AI Chat with CodeIgniter, Ollama, and Neuron AI","description":"Learn how to build a modern AI-powered chat application with CodeIgniter 4, featuring real-time SSE streaming, persistent conversation history, and function calling capabilities.","keywords":["codeigniter4","php","ai","ollama","neuron-ai","sse","streaming"],"articleBody":"AI capabilities are no longer a luxury feature - they are increasingly expected in modern web applications. As frameworks across the ecosystem rush to integrate LLM functionality (Laravel introduced Laravel AI, Python has LangChain, JavaScript has Vercel AI SDK), it is easy to assume you need to switch stacks to build AI-powered features. You do not.\nCodeIgniter 4 can build the same modern, AI-driven applications as any other framework. Large language models are impressive, but wiring one into a real web application - with streaming, conversation memory, and tool usage - takes more than a single API call. In this article we build exactly that: a chat interface powered by a local LLM, starting from the simplest possible request/response and layering on complexity one phase at a time. The result is a fully functional AI chat application in PHP with CodeIgniter 4.\nThe stack:\nOllama - a local LLM server. Install it, pull a model (ollama pull qwen3:1.7b), and you have an OpenAI-compatible API running on localhost:11434. No API keys, no cloud bills. Neuron AI - a PHP library for building AI agents. It is framework-agnostic and handles provider abstraction, streaming, chat history, and tool calling. Think of it as the glue between your application and the LLM. CodeIgniter 4.8-dev - specifically its new SSEResponse class, which makes Server-Sent Events a first-class response type. We will treat it as a black box in this article: you return it from a controller and it handles headers, output buffering, and flushing. By the end we will have: JSON responses, real-time SSE streaming, persistent chat history, and function calling.\nPhase 1: Simple Request/Response The Agent Every Neuron AI integration starts with an agent class. An agent defines three things: which LLM provider to use, what system instructions the model receives, and (optionally) what tools it can call. Here is the minimal version:\n\u003c?php namespace App\\Neuron; use NeuronAI\\Agent; use NeuronAI\\Providers\\AIProviderInterface; use NeuronAI\\Providers\\Ollama\\Ollama; class MyAgent extends Agent { protected function provider(): AIProviderInterface { return new Ollama( url: 'http://localhost:11434/api/', model: 'qwen3:1.7b', ); } public function instructions(): string { return 'You are a friendly AI assistant.'; } } Ollama is one of several providers Neuron AI supports (OpenAI, Anthropic, etc.). The interface is identical regardless of which one you pick.\nThe Controller The controller receives a message, passes it to the agent, and returns JSON:\nuse App\\Neuron\\MyAgent; use NeuronAI\\Chat\\Messages\\UserMessage; public function send() { $userMessage = $this-\u003erequest-\u003egetJsonVar('message'); $response = MyAgent::make()-\u003echat( new UserMessage($userMessage), ); return $this-\u003eresponse-\u003esetJSON([ 'reply' =\u003e $response-\u003egetContent(), ]); } MyAgent::make() instantiates the agent with all its configured defaults. chat() sends the message and blocks until the full response is available.\nThe Frontend A minimal fetch call:\nconst res = await fetch('/chat/send', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ message: input.value }), }); const data = await res.json(); displayMessage(data.reply); This works, but the user stares at a blank screen until the entire response is generated. For a local 1.7B model that might be a few seconds. For a larger model or a complex prompt, it could be much longer.\nPhase 2: Real-Time Streaming with SSE Why Streaming Matters LLMs generate text token by token. With a blocking chat() call, all those tokens buffer on the server until the last one arrives. Streaming sends each token to the browser as it is produced. The user sees words appear in real time, which makes even slow models feel responsive.\nSwitching to SSE Neuron AI provides a stream() method that returns an iterator of chunks instead of a complete response. On the CodeIgniter side, we return an SSEResponse instead of a JSON response.\nSSEResponse takes a closure. When the framework calls send(), it sets up the correct headers (text/event-stream, Cache-Control: no-cache, etc.), clears output buffers, closes the session (so other requests are not blocked), and then invokes your closure. Inside the closure you call $sse-\u003eevent() to push data to the client.\nuse App\\Neuron\\MyAgent; use CodeIgniter\\HTTP\\SSEResponse; use NeuronAI\\Chat\\Messages\\UserMessage; public function send(): ResponseInterface|SSEResponse { $userMessage = $this-\u003erequest-\u003egetJsonVar('message'); return new SSEResponse(function (SSEResponse $sse) use ($userMessage) { $stream = MyAgent::make()-\u003estream( new UserMessage($userMessage), ); foreach ($stream as $chunk) { if (! $sse-\u003eevent(data: ['text' =\u003e $chunk])) { break; // client disconnected } } $sse-\u003eevent(data: '[DONE]'); }); } A few things to notice:\nevent() accepts arrays (auto-JSON-encoded) or strings. It returns bool - false means the client has disconnected, so you should stop generating. The [DONE] sentinel tells the frontend the stream is complete. Parsing SSE on the Frontend We use the Fetch API with a ReadableStream reader rather than EventSource, because we need to send a POST body and custom headers.\nconst res = await fetch('/chat/send', { method: 'POST', headers: { 'Content-Type': 'application/json', 'X-Requested-With': 'XMLHttpRequest', }, body: JSON.stringify({ message }), }); const reader = res.body.getReader(); const decoder = new TextDecoder(); let buffer = ''; let rawText = ''; while (true) { const { done, value } = await reader.read(); if (done) break; buffer += decoder.decode(value, { stream: true }); const lines = buffer.split('\\n'); buffer = lines.pop(); // keep incomplete line in buffer for (const line of lines) { if (!line.startsWith('data: ')) continue; const payload = line.slice(6); if (payload === '[DONE]') continue; const { text } = JSON.parse(payload); rawText += text; contentEl.innerHTML = renderMarkdown(rawText); } } The SSE protocol is line-based: each field is field: value\\n, and events are separated by a blank line. We split on newlines, look for data: lines, and append each text chunk to the growing response. The result is a ChatGPT-like streaming effect with surprisingly little code.\nPhase 3: Chat History The Problem Every call to stream() (or chat()) is stateless. The agent has no memory of previous messages. Ask it “What did I just say?” and it will have no idea.\nNeuron AI’s Chat History Interface Neuron AI defines a ChatHistoryInterface that you can implement with any storage backend - files, Redis, a database. When an agent has a chat history configured, Neuron AI automatically loads previous messages before sending the prompt and saves new messages after the response completes.\nA CodeIgniter-Backed Implementation We need a database table for messages:\nColumn Type id BIGINT (PK) thread_id INT role VARCHAR content LONGTEXT meta JSON And a standard CodeIgniter model:\n\u003c?php namespace App\\Models; use CodeIgniter\\Model; use App\\Entities\\ChatMessage; class ChatMessageModel extends Model { protected $table = 'chat_messages'; protected $returnType = ChatMessage::class; protected $allowedFields = ['thread_id', 'role', 'content', 'meta']; protected $useTimestamps = true; protected array $casts = ['meta' =\u003e 'json-array']; } The chat history adapter extends Neuron AI’s AbstractChatHistory and bridges it to the model:\n\u003c?php namespace App\\Neuron\\Chat\\History; use NeuronAI\\Chat\\History\\AbstractChatHistory; use NeuronAI\\Chat\\History\\ChatHistoryInterface; use NeuronAI\\Chat\\Messages\\Message; use NeuronAI\\Chat\\Messages\\ToolCallMessage; use NeuronAI\\Chat\\Messages\\ToolCallResultMessage; class CodeIgniterChatHistory extends AbstractChatHistory { public function __construct( protected string $threadId, protected string $modelClass, int $contextWindow = 50000, ) { parent::__construct($contextWindow); $this-\u003eload(); } protected function load(): void { $messages = model($this-\u003emodelClass) -\u003ewhere('thread_id', $this-\u003ethreadId) -\u003eorderBy('id') -\u003efindAll(); $messages = array_map($this-\u003erecordToArray(...), $messages); if ($messages !== []) { $this-\u003ehistory = $this-\u003edeserializeMessages($messages); } } protected function onNewMessage(Message $message): void { // Tool messages are transient - don't persist them if ($message instanceof ToolCallMessage || $message instanceof ToolCallResultMessage) { return; } model($this-\u003emodelClass)-\u003einsert([ 'thread_id' =\u003e $this-\u003ethreadId, 'role' =\u003e $message-\u003egetRole(), 'content' =\u003e $message-\u003egetContent(), 'meta' =\u003e $this-\u003eserializeMessageMeta($message), ]); } // ... see the rest on the GitHub repo } The key decisions here:\nThread ID groups messages into conversations. contextWindow limits how many tokens of history Neuron AI includes in the prompt. When the window is exceeded, older messages are trimmed automatically via onTrimHistory(). Tool messages are not persisted. They are ephemeral - the LLM generates them during a single request and they have no value in future conversations. Wiring It Into the Agent The agent now accepts a thread ID and declares a chatHistory() method:\nclass MyAgent extends Agent { public function __construct(private readonly string $threadId) { } protected function provider(): AIProviderInterface { return new Ollama( url: 'http://localhost:11434/api/', model: 'qwen3:1.7b', ); } public function instructions(): string { return 'You are a friendly AI assistant.'; } protected function chatHistory(): ChatHistoryInterface { return new CodeIgniterChatHistory( threadId: $this-\u003ethreadId, modelClass: ChatMessageModel::class, contextWindow: 50000, ); } } The controller passes the thread ID when creating the agent:\n$stream = MyAgent::make($threadId)-\u003estream( new UserMessage($userMessage), ); That is all. Neuron AI handles loading prior messages into the prompt and saving the user message and assistant response to the database. The conversation now survives page reloads.\nPhase 4: Tool Usage What Tools Are Tools (also called function calling) let the LLM request that your application execute a function and return the result. The model does not run code itself - it emits a structured “call this function with these arguments” message, your code executes it, and you feed the result back into the conversation.\nThis is how AI agents do things like check the weather, query a database, or (like in our case) look up the current date. Because models have no idea about the present time, this is the easiest way to demonstrate how tools work.\nDefining Tools in the Agent Add a tools() method to the agent:\nuse CodeIgniter\\I18n\\Time; use NeuronAI\\Tools\\Tool; protected function tools(): array { return [ Tool::make( 'get_current_date', 'Retrieve current date and time.', )-\u003esetCallable(function () { return Time::now()-\u003etoDateTimeString(); }), Tool::make( 'get_current_weekday', 'Retrieve current weekday name.', )-\u003esetCallable(function () { return Time::now()-\u003eformat('l'); }), ]; } Each tool has a name, a description (which the LLM reads to decide when to use it), and a callable that produces the result. You can also give the system prompt hints about when to use each tool:\nuse NeuronAI\\SystemPrompt; public function instructions(): string { return (string) new SystemPrompt( background: [ 'You are a friendly AI Agent created with Neuron framework.', ], toolsUsage: [ 'To get current date and time, use get_current_date', 'To get current weekday use get_current_weekday', ], ); } Handling Tool Calls in the Stream With tools enabled, the stream no longer yields only text strings. It can also yield ToolCallMessage (the model wants to call a function) and ToolCallResultMessage (the function has returned a result). We need to handle these in the controller:\nuse NeuronAI\\Chat\\Messages\\ToolCallMessage; use NeuronAI\\Chat\\Messages\\ToolCallResultMessage; return new SSEResponse(function (SSEResponse $sse) use ($userMessage, $threadId) { $stream = MyAgent::make($threadId)-\u003estream( new UserMessage($userMessage), ); foreach ($stream as $chunk) { // Result messages are internal - skip them if ($chunk instanceof ToolCallResultMessage) { continue; } // Tool call messages carry the tool name(s) if ($chunk instanceof ToolCallMessage) { $tools = array_map( static fn ($tool) =\u003e $tool-\u003egetName(), $chunk-\u003egetTools() ); if (! $sse-\u003eevent(data: ['tools' =\u003e $tools], event: 'tool')) { break; } continue; } // Regular text chunk if ($chunk !== '') { if (! $sse-\u003eevent(data: ['text' =\u003e $chunk])) { break; } } } $sse-\u003eevent(data: '[DONE]'); }); Notice the named event: event: 'tool'. This lets the frontend distinguish tool calls from text chunks without inspecting the payload.\nShowing Tool Usage on the Frontend On the frontend, we check for the event: line before parsing data::\nlet currentEvent = null; for (const line of lines) { if (line.startsWith('event: ')) { currentEvent = line.slice(7).trim(); continue; } if (!line.startsWith('data: ')) continue; const payload = line.slice(6); if (payload === '[DONE]') { currentEvent = null; continue; } const parsed = JSON.parse(payload); if (currentEvent === 'tool' \u0026\u0026 parsed.tools) { toolIndicator.textContent = 'Using tools: ' + parsed.tools.join(', '); toolIndicator.classList.remove('hidden'); currentEvent = null; continue; } currentEvent = null; // Regular text chunk rawText += parsed.text; contentEl.innerHTML = renderMarkdown(rawText); } When the user asks “What day is it?”, they see a brief “Using tools: get_current_date, get_current_weekday” indicator while the model calls the functions, followed by the streamed answer that includes the actual date. The whole round-trip - model decides to call a tool, PHP executes it, result goes back to the model, model generates the final answer - happens within a single SSE stream.\nWhat We Built Four phases, each building on the last:\nJSON request/response - the simplest thing that works. A POST, a blocking chat() call, a JSON reply. SSE streaming - switch to stream() and SSEResponse, get real-time token delivery with connection-abort detection. Chat history - implement ChatHistoryInterface backed by a CodeIgniter model, give each conversation a thread ID, and the agent remembers everything. Tool usage - define callable tools in the agent, filter the stream for ToolCallMessage types, and let the LLM interact with your application. The progression shows a pattern: Neuron AI handles the AI complexity (prompt assembly, streaming protocol, tool orchestration), SSEResponse handles the HTTP complexity (headers, buffering, flushing), and your application code stays focused on business logic - which tools to expose, how to store history, what to show the user.\nFrom here you could add RAG (retrieval-augmented generation) to let the agent search your documents, multi-agent workflows where agents delegate to each other, image generation, or any of the other capabilities Neuron AI supports. The foundation is the same: an agent, a provider, and a stream.\nSource Code The complete codebase for this chat application, including migrations, and frontend code, is available at github.com/michalsn/codeigniter-neuron-ai-chat.\n","wordCount":"2115","inLanguage":"en","datePublished":"2026-02-12T19:55:21+01:00","dateModified":"2026-02-12T19:55:21+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://michalsn.dev/posts/building-an-ai-chat-with-codeigniter-ollama-and-neuron-ai/"},"publisher":{"@type":"Organization","name":"michalsn","logo":{"@type":"ImageObject","url":"https://michalsn.dev/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://michalsn.dev/ accesskey=h title="michalsn (Alt + H)">michalsn</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michalsn.dev/projects title=Projects><span>Projects</span></a></li><li><a href=https://michalsn.dev/archives title=Archive><span>Archive</span></a></li><li><a href=https://michalsn.dev/tags title=Tags><span>Tags</span></a></li><li><a href=https://michalsn.dev/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Building an AI Chat with CodeIgniter, Ollama, and Neuron AI</h1><div class=post-meta><span title='2026-02-12 19:55:21 +0100 +0100'>February 12, 2026</span>&nbsp;·&nbsp;<span>10 min</span></div></header><div class=post-content><p>AI capabilities are no longer a luxury feature - they are increasingly expected in modern web applications. As frameworks across the ecosystem rush to integrate LLM functionality (Laravel introduced Laravel AI, Python has LangChain, JavaScript has Vercel AI SDK), it is easy to assume you need to switch stacks to build AI-powered features. You do not.</p><p>CodeIgniter 4 can build the same modern, AI-driven applications as any other framework. Large language models are impressive, but wiring one into a real web application - with streaming, conversation memory, and tool usage - takes more than a single API call. In this article we build exactly that: a chat interface powered by a local LLM, starting from the simplest possible request/response and layering on complexity one phase at a time. The result is a fully functional AI chat application in PHP with CodeIgniter 4.</p><p>The stack:</p><ul><li><strong><a href=https://ollama.com/>Ollama</a></strong> - a local LLM server. Install it, pull a model (<code>ollama pull qwen3:1.7b</code>), and you have an OpenAI-compatible API running on <code>localhost:11434</code>. No API keys, no cloud bills.</li><li><strong><a href=https://neuronai.dev/>Neuron AI</a></strong> - a PHP library for building AI agents. It is framework-agnostic and handles provider abstraction, streaming, chat history, and tool calling. Think of it as the glue between your application and the LLM.</li><li><strong><a href=https://codeigniter.com/user_guide/installation/installing_composer.html#next-minor-version>CodeIgniter 4.8-dev</a></strong> - specifically its new <code>SSEResponse</code> class, which makes Server-Sent Events a first-class response type. We will treat it as a black box in this article: you return it from a controller and it handles headers, output buffering, and flushing.</li></ul><p>By the end we will have: JSON responses, real-time SSE streaming, persistent chat history, and function calling.</p><h2 id=phase-1-simple-requestresponse>Phase 1: Simple Request/Response<a hidden class=anchor aria-hidden=true href=#phase-1-simple-requestresponse>#</a></h2><h3 id=the-agent>The Agent<a hidden class=anchor aria-hidden=true href=#the-agent>#</a></h3><p>Every Neuron AI integration starts with an agent class. An agent defines three things: which LLM provider to use, what system instructions the model receives, and (optionally) what tools it can call. Here is the minimal version:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#f92672>&lt;?</span><span style=color:#a6e22e>php</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>namespace</span> <span style=color:#a6e22e>App\Neuron</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Agent</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Providers\AIProviderInterface</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Providers\Ollama\Ollama</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyAgent</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>Agent</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>provider</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>AIProviderInterface</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>Ollama</span>(
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>url</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;http://localhost:11434/api/&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;qwen3:1.7b&#39;</span>,
</span></span><span style=display:flex><span>        );
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>instructions</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>string</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;You are a friendly AI assistant.&#39;</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>Ollama</code> is one of several providers Neuron AI supports (OpenAI, Anthropic, etc.). The interface is identical regardless of which one you pick.</p><h3 id=the-controller>The Controller<a hidden class=anchor aria-hidden=true href=#the-controller>#</a></h3><p>The controller receives a message, passes it to the agent, and returns JSON:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>App\Neuron\MyAgent</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\UserMessage</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>send</span>()
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    $userMessage <span style=color:#f92672>=</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>request</span><span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getJsonVar</span>(<span style=color:#e6db74>&#39;message&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    $response <span style=color:#f92672>=</span> <span style=color:#a6e22e>MyAgent</span><span style=color:#f92672>::</span><span style=color:#a6e22e>make</span>()<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>chat</span>(
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>UserMessage</span>($userMessage),
</span></span><span style=display:flex><span>    );
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>response</span><span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>setJSON</span>([
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;reply&#39;</span> <span style=color:#f92672>=&gt;</span> $response<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getContent</span>(),
</span></span><span style=display:flex><span>    ]);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>MyAgent::make()</code> instantiates the agent with all its configured defaults. <code>chat()</code> sends the message and blocks until the full response is available.</p><h3 id=the-frontend>The Frontend<a hidden class=anchor aria-hidden=true href=#the-frontend>#</a></h3><p>A minimal fetch call:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>res</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>fetch</span>(<span style=color:#e6db74>&#39;/chat/send&#39;</span>, {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>method</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;POST&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>headers</span><span style=color:#f92672>:</span> { <span style=color:#e6db74>&#39;Content-Type&#39;</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;application/json&#39;</span> },
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>body</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>stringify</span>({ <span style=color:#a6e22e>message</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>input</span>.<span style=color:#a6e22e>value</span> }),
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>data</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>json</span>();
</span></span><span style=display:flex><span><span style=color:#a6e22e>displayMessage</span>(<span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>reply</span>);
</span></span></code></pre></div><p>This works, but the user stares at a blank screen until the entire response is generated. For a local 1.7B model that might be a few seconds. For a larger model or a complex prompt, it could be much longer.</p><h2 id=phase-2-real-time-streaming-with-sse>Phase 2: Real-Time Streaming with SSE<a hidden class=anchor aria-hidden=true href=#phase-2-real-time-streaming-with-sse>#</a></h2><h3 id=why-streaming-matters>Why Streaming Matters<a hidden class=anchor aria-hidden=true href=#why-streaming-matters>#</a></h3><p>LLMs generate text token by token. With a blocking <code>chat()</code> call, all those tokens buffer on the server until the last one arrives. Streaming sends each token to the browser as it is produced. The user sees words appear in real time, which makes even slow models feel responsive.</p><h3 id=switching-to-sse>Switching to SSE<a hidden class=anchor aria-hidden=true href=#switching-to-sse>#</a></h3><p>Neuron AI provides a <code>stream()</code> method that returns an iterator of chunks instead of a complete response. On the CodeIgniter side, we return an <code>SSEResponse</code> instead of a JSON response.</p><p><code>SSEResponse</code> takes a closure. When the framework calls <code>send()</code>, it sets up the correct headers (<code>text/event-stream</code>, <code>Cache-Control: no-cache</code>, etc.), clears output buffers, closes the session (so other requests are not blocked), and then invokes your closure. Inside the closure you call <code>$sse->event()</code> to push data to the client.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>App\Neuron\MyAgent</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>CodeIgniter\HTTP\SSEResponse</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\UserMessage</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>send</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>ResponseInterface</span><span style=color:#f92672>|</span><span style=color:#a6e22e>SSEResponse</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    $userMessage <span style=color:#f92672>=</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>request</span><span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getJsonVar</span>(<span style=color:#e6db74>&#39;message&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>SSEResponse</span>(<span style=color:#66d9ef>function</span> (<span style=color:#a6e22e>SSEResponse</span> $sse) <span style=color:#66d9ef>use</span> ($userMessage) {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        $stream <span style=color:#f92672>=</span> <span style=color:#a6e22e>MyAgent</span><span style=color:#f92672>::</span><span style=color:#a6e22e>make</span>()<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>stream</span>(
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>UserMessage</span>($userMessage),
</span></span><span style=display:flex><span>        );
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>foreach</span> ($stream <span style=color:#66d9ef>as</span> $chunk) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span> $sse<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>event</span>(<span style=color:#a6e22e>data</span><span style=color:#f92672>:</span> [<span style=color:#e6db74>&#39;text&#39;</span> <span style=color:#f92672>=&gt;</span> $chunk])) {
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>; <span style=color:#75715e>// client disconnected
</span></span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        $sse<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>event</span>(<span style=color:#a6e22e>data</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;[DONE]&#39;</span>);
</span></span><span style=display:flex><span>    });
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>A few things to notice:</p><ul><li><code>event()</code> accepts arrays (auto-JSON-encoded) or strings.</li><li>It returns <code>bool</code> - <code>false</code> means the client has disconnected, so you should stop generating.</li><li>The <code>[DONE]</code> sentinel tells the frontend the stream is complete.</li></ul><h3 id=parsing-sse-on-the-frontend>Parsing SSE on the Frontend<a hidden class=anchor aria-hidden=true href=#parsing-sse-on-the-frontend>#</a></h3><p>We use the Fetch API with a <code>ReadableStream</code> reader rather than <code>EventSource</code>, because we need to send a POST body and custom headers.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>res</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>fetch</span>(<span style=color:#e6db74>&#39;/chat/send&#39;</span>, {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>method</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;POST&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>headers</span><span style=color:#f92672>:</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;Content-Type&#39;</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;application/json&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;X-Requested-With&#39;</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;XMLHttpRequest&#39;</span>,
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>body</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>stringify</span>({ <span style=color:#a6e22e>message</span> }),
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>reader</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>body</span>.<span style=color:#a6e22e>getReader</span>();
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>decoder</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>TextDecoder</span>();
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> <span style=color:#a6e22e>buffer</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> <span style=color:#a6e22e>rawText</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> (<span style=color:#66d9ef>true</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>done</span>, <span style=color:#a6e22e>value</span> } <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>reader</span>.<span style=color:#a6e22e>read</span>();
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>done</span>) <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>buffer</span> <span style=color:#f92672>+=</span> <span style=color:#a6e22e>decoder</span>.<span style=color:#a6e22e>decode</span>(<span style=color:#a6e22e>value</span>, { <span style=color:#a6e22e>stream</span><span style=color:#f92672>:</span> <span style=color:#66d9ef>true</span> });
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>lines</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>buffer</span>.<span style=color:#a6e22e>split</span>(<span style=color:#e6db74>&#39;\n&#39;</span>);
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>buffer</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>lines</span>.<span style=color:#a6e22e>pop</span>(); <span style=color:#75715e>// keep incomplete line in buffer
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>line</span> <span style=color:#66d9ef>of</span> <span style=color:#a6e22e>lines</span>) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>startsWith</span>(<span style=color:#e6db74>&#39;data: &#39;</span>)) <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>payload</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>slice</span>(<span style=color:#ae81ff>6</span>);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>payload</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#39;[DONE]&#39;</span>) <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>text</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>payload</span>);
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>rawText</span> <span style=color:#f92672>+=</span> <span style=color:#a6e22e>text</span>;
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>contentEl</span>.<span style=color:#a6e22e>innerHTML</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>renderMarkdown</span>(<span style=color:#a6e22e>rawText</span>);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The SSE protocol is line-based: each field is <code>field: value\n</code>, and events are separated by a blank line. We split on newlines, look for <code>data:</code> lines, and append each text chunk to the growing response. The result is a ChatGPT-like streaming effect with surprisingly little code.</p><h2 id=phase-3-chat-history>Phase 3: Chat History<a hidden class=anchor aria-hidden=true href=#phase-3-chat-history>#</a></h2><h3 id=the-problem>The Problem<a hidden class=anchor aria-hidden=true href=#the-problem>#</a></h3><p>Every call to <code>stream()</code> (or <code>chat()</code>) is stateless. The agent has no memory of previous messages. Ask it &ldquo;What did I just say?&rdquo; and it will have no idea.</p><h3 id=neuron-ais-chat-history-interface>Neuron AI&rsquo;s Chat History Interface<a hidden class=anchor aria-hidden=true href=#neuron-ais-chat-history-interface>#</a></h3><p>Neuron AI defines a <code>ChatHistoryInterface</code> that you can implement with any storage backend - files, Redis, a database. When an agent has a chat history configured, Neuron AI automatically loads previous messages before sending the prompt and saves new messages after the response completes.</p><h3 id=a-codeigniter-backed-implementation>A CodeIgniter-Backed Implementation<a hidden class=anchor aria-hidden=true href=#a-codeigniter-backed-implementation>#</a></h3><p>We need a database table for messages:</p><table><thead><tr><th>Column</th><th>Type</th></tr></thead><tbody><tr><td>id</td><td>BIGINT (PK)</td></tr><tr><td>thread_id</td><td>INT</td></tr><tr><td>role</td><td>VARCHAR</td></tr><tr><td>content</td><td>LONGTEXT</td></tr><tr><td>meta</td><td>JSON</td></tr></tbody></table><p>And a standard CodeIgniter model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#f92672>&lt;?</span><span style=color:#a6e22e>php</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>namespace</span> <span style=color:#a6e22e>App\Models</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>CodeIgniter\Model</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>App\Entities\ChatMessage</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ChatMessageModel</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>Model</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> $table            <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;chat_messages&#39;</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> $returnType       <span style=color:#f92672>=</span> <span style=color:#a6e22e>ChatMessage</span><span style=color:#f92672>::</span><span style=color:#a6e22e>class</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> $allowedFields    <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;thread_id&#39;</span>, <span style=color:#e6db74>&#39;role&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>, <span style=color:#e6db74>&#39;meta&#39;</span>];
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> $useTimestamps    <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>array</span> $casts      <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;meta&#39;</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;json-array&#39;</span>];
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The chat history adapter extends Neuron AI&rsquo;s <code>AbstractChatHistory</code> and bridges it to the model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#f92672>&lt;?</span><span style=color:#a6e22e>php</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>namespace</span> <span style=color:#a6e22e>App\Neuron\Chat\History</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\History\AbstractChatHistory</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\History\ChatHistoryInterface</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\Message</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\ToolCallMessage</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\ToolCallResultMessage</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CodeIgniterChatHistory</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>AbstractChatHistory</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>__construct</span>(
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>protected</span> <span style=color:#a6e22e>string</span> $threadId,
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>protected</span> <span style=color:#a6e22e>string</span> $modelClass,
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>int</span> $contextWindow <span style=color:#f92672>=</span> <span style=color:#ae81ff>50000</span>,
</span></span><span style=display:flex><span>    ) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>parent</span><span style=color:#f92672>::</span><span style=color:#a6e22e>__construct</span>($contextWindow);
</span></span><span style=display:flex><span>        $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>load</span>();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>load</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>void</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        $messages <span style=color:#f92672>=</span> <span style=color:#a6e22e>model</span>($this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>modelClass</span>)
</span></span><span style=display:flex><span>            <span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>where</span>(<span style=color:#e6db74>&#39;thread_id&#39;</span>, $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>threadId</span>)
</span></span><span style=display:flex><span>            <span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>orderBy</span>(<span style=color:#e6db74>&#39;id&#39;</span>)
</span></span><span style=display:flex><span>            <span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>findAll</span>();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        $messages <span style=color:#f92672>=</span> <span style=color:#a6e22e>array_map</span>($this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>recordToArray</span>(<span style=color:#f92672>...</span>), $messages);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ($messages <span style=color:#f92672>!==</span> []) {
</span></span><span style=display:flex><span>            $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>history</span> <span style=color:#f92672>=</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>deserializeMessages</span>($messages);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>onNewMessage</span>(<span style=color:#a6e22e>Message</span> $message)<span style=color:#f92672>:</span> <span style=color:#a6e22e>void</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Tool messages are transient - don&#39;t persist them
</span></span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ($message <span style=color:#a6e22e>instanceof</span> <span style=color:#a6e22e>ToolCallMessage</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>||</span> $message <span style=color:#a6e22e>instanceof</span> <span style=color:#a6e22e>ToolCallResultMessage</span>) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>model</span>($this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>modelClass</span>)<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>insert</span>([
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;thread_id&#39;</span> <span style=color:#f92672>=&gt;</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>threadId</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;role&#39;</span>      <span style=color:#f92672>=&gt;</span> $message<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getRole</span>(),
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;content&#39;</span>   <span style=color:#f92672>=&gt;</span> $message<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getContent</span>(),
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;meta&#39;</span>      <span style=color:#f92672>=&gt;</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>serializeMessageMeta</span>($message),
</span></span><span style=display:flex><span>        ]);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ... see the rest on the GitHub repo
</span></span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The key decisions here:</p><ul><li><strong>Thread ID</strong> groups messages into conversations.</li><li><strong><code>contextWindow</code></strong> limits how many tokens of history Neuron AI includes in the prompt. When the window is exceeded, older messages are trimmed automatically via <code>onTrimHistory()</code>.</li><li><strong>Tool messages are not persisted.</strong> They are ephemeral - the LLM generates them during a single request and they have no value in future conversations.</li></ul><h3 id=wiring-it-into-the-agent>Wiring It Into the Agent<a hidden class=anchor aria-hidden=true href=#wiring-it-into-the-agent>#</a></h3><p>The agent now accepts a thread ID and declares a <code>chatHistory()</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyAgent</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>Agent</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>__construct</span>(<span style=color:#66d9ef>private</span> <span style=color:#a6e22e>readonly</span> <span style=color:#a6e22e>string</span> $threadId)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>provider</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>AIProviderInterface</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>Ollama</span>(
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>url</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;http://localhost:11434/api/&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;qwen3:1.7b&#39;</span>,
</span></span><span style=display:flex><span>        );
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>instructions</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>string</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;You are a friendly AI assistant.&#39;</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>chatHistory</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>ChatHistoryInterface</span>
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>CodeIgniterChatHistory</span>(
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>threadId</span><span style=color:#f92672>:</span> $this<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>threadId</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>modelClass</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>ChatMessageModel</span><span style=color:#f92672>::</span><span style=color:#a6e22e>class</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>contextWindow</span><span style=color:#f92672>:</span> <span style=color:#ae81ff>50000</span>,
</span></span><span style=display:flex><span>        );
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The controller passes the thread ID when creating the agent:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span>$stream <span style=color:#f92672>=</span> <span style=color:#a6e22e>MyAgent</span><span style=color:#f92672>::</span><span style=color:#a6e22e>make</span>($threadId)<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>stream</span>(
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>UserMessage</span>($userMessage),
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>That is all. Neuron AI handles loading prior messages into the prompt and saving the user message and assistant response to the database. The conversation now survives page reloads.</p><h2 id=phase-4-tool-usage>Phase 4: Tool Usage<a hidden class=anchor aria-hidden=true href=#phase-4-tool-usage>#</a></h2><h3 id=what-tools-are>What Tools Are<a hidden class=anchor aria-hidden=true href=#what-tools-are>#</a></h3><p>Tools (also called function calling) let the LLM request that your application execute a function and return the result. The model does not run code itself - it emits a structured &ldquo;call this function with these arguments&rdquo; message, your code executes it, and you feed the result back into the conversation.</p><p>This is how AI agents do things like check the weather, query a database, or (like in our case) look up the current date. Because models have no idea about the present time, this is the easiest way to demonstrate how tools work.</p><h3 id=defining-tools-in-the-agent>Defining Tools in the Agent<a hidden class=anchor aria-hidden=true href=#defining-tools-in-the-agent>#</a></h3><p>Add a <code>tools()</code> method to the agent:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>CodeIgniter\I18n\Time</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Tools\Tool</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>protected</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>tools</span>()<span style=color:#f92672>:</span> <span style=color:#66d9ef>array</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>Tool</span><span style=color:#f92672>::</span><span style=color:#a6e22e>make</span>(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;get_current_date&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;Retrieve current date and time.&#39;</span>,
</span></span><span style=display:flex><span>        )<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>setCallable</span>(<span style=color:#66d9ef>function</span> () {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>Time</span><span style=color:#f92672>::</span><span style=color:#a6e22e>now</span>()<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>toDateTimeString</span>();
</span></span><span style=display:flex><span>        }),
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>Tool</span><span style=color:#f92672>::</span><span style=color:#a6e22e>make</span>(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;get_current_weekday&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;Retrieve current weekday name.&#39;</span>,
</span></span><span style=display:flex><span>        )<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>setCallable</span>(<span style=color:#66d9ef>function</span> () {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>Time</span><span style=color:#f92672>::</span><span style=color:#a6e22e>now</span>()<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>format</span>(<span style=color:#e6db74>&#39;l&#39;</span>);
</span></span><span style=display:flex><span>        }),
</span></span><span style=display:flex><span>    ];
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Each tool has a name, a description (which the LLM reads to decide when to use it), and a callable that produces the result. You can also give the system prompt hints about when to use each tool:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\SystemPrompt</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>instructions</span>()<span style=color:#f92672>:</span> <span style=color:#a6e22e>string</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (<span style=color:#a6e22e>string</span>) <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>SystemPrompt</span>(
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>background</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;You are a friendly AI Agent created with Neuron framework.&#39;</span>,
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>toolsUsage</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;To get current date and time, use get_current_date&#39;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;To get current weekday use get_current_weekday&#39;</span>,
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>    );
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=handling-tool-calls-in-the-stream>Handling Tool Calls in the Stream<a hidden class=anchor aria-hidden=true href=#handling-tool-calls-in-the-stream>#</a></h3><p>With tools enabled, the stream no longer yields only text strings. It can also yield <code>ToolCallMessage</code> (the model wants to call a function) and <code>ToolCallResultMessage</code> (the function has returned a result). We need to handle these in the controller:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-php data-lang=php><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\ToolCallMessage</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> <span style=color:#a6e22e>NeuronAI\Chat\Messages\ToolCallResultMessage</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>return</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>SSEResponse</span>(<span style=color:#66d9ef>function</span> (<span style=color:#a6e22e>SSEResponse</span> $sse) <span style=color:#66d9ef>use</span> ($userMessage, $threadId) {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    $stream <span style=color:#f92672>=</span> <span style=color:#a6e22e>MyAgent</span><span style=color:#f92672>::</span><span style=color:#a6e22e>make</span>($threadId)<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>stream</span>(
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>UserMessage</span>($userMessage),
</span></span><span style=display:flex><span>    );
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>foreach</span> ($stream <span style=color:#66d9ef>as</span> $chunk) {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Result messages are internal - skip them
</span></span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ($chunk <span style=color:#a6e22e>instanceof</span> <span style=color:#a6e22e>ToolCallResultMessage</span>) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Tool call messages carry the tool name(s)
</span></span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ($chunk <span style=color:#a6e22e>instanceof</span> <span style=color:#a6e22e>ToolCallMessage</span>) {
</span></span><span style=display:flex><span>            $tools <span style=color:#f92672>=</span> <span style=color:#a6e22e>array_map</span>(
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>static</span> <span style=color:#a6e22e>fn</span> ($tool) <span style=color:#f92672>=&gt;</span> $tool<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getName</span>(),
</span></span><span style=display:flex><span>                $chunk<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>getTools</span>()
</span></span><span style=display:flex><span>            );
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span> $sse<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>event</span>(<span style=color:#a6e22e>data</span><span style=color:#f92672>:</span> [<span style=color:#e6db74>&#39;tools&#39;</span> <span style=color:#f92672>=&gt;</span> $tools], <span style=color:#a6e22e>event</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;tool&#39;</span>)) {
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Regular text chunk
</span></span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ($chunk <span style=color:#f92672>!==</span> <span style=color:#e6db74>&#39;&#39;</span>) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span> $sse<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>event</span>(<span style=color:#a6e22e>data</span><span style=color:#f92672>:</span> [<span style=color:#e6db74>&#39;text&#39;</span> <span style=color:#f92672>=&gt;</span> $chunk])) {
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    $sse<span style=color:#f92672>-&gt;</span><span style=color:#a6e22e>event</span>(<span style=color:#a6e22e>data</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;[DONE]&#39;</span>);
</span></span><span style=display:flex><span>});
</span></span></code></pre></div><p>Notice the named event: <code>event: 'tool'</code>. This lets the frontend distinguish tool calls from text chunks without inspecting the payload.</p><h3 id=showing-tool-usage-on-the-frontend>Showing Tool Usage on the Frontend<a hidden class=anchor aria-hidden=true href=#showing-tool-usage-on-the-frontend>#</a></h3><p>On the frontend, we check for the <code>event:</code> line before parsing <code>data:</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>let</span> <span style=color:#a6e22e>currentEvent</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>null</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>line</span> <span style=color:#66d9ef>of</span> <span style=color:#a6e22e>lines</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>startsWith</span>(<span style=color:#e6db74>&#39;event: &#39;</span>)) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>currentEvent</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>slice</span>(<span style=color:#ae81ff>7</span>).<span style=color:#a6e22e>trim</span>();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>startsWith</span>(<span style=color:#e6db74>&#39;data: &#39;</span>)) <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>payload</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>slice</span>(<span style=color:#ae81ff>6</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>payload</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#39;[DONE]&#39;</span>) { <span style=color:#a6e22e>currentEvent</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>null</span>; <span style=color:#66d9ef>continue</span>; }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>parsed</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>payload</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>currentEvent</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#39;tool&#39;</span> <span style=color:#f92672>&amp;&amp;</span> <span style=color:#a6e22e>parsed</span>.<span style=color:#a6e22e>tools</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>toolIndicator</span>.<span style=color:#a6e22e>textContent</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Using tools: &#39;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>parsed</span>.<span style=color:#a6e22e>tools</span>.<span style=color:#a6e22e>join</span>(<span style=color:#e6db74>&#39;, &#39;</span>);
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>toolIndicator</span>.<span style=color:#a6e22e>classList</span>.<span style=color:#a6e22e>remove</span>(<span style=color:#e6db74>&#39;hidden&#39;</span>);
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>currentEvent</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>null</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>currentEvent</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>null</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Regular text chunk
</span></span></span><span style=display:flex><span>    <span style=color:#a6e22e>rawText</span> <span style=color:#f92672>+=</span> <span style=color:#a6e22e>parsed</span>.<span style=color:#a6e22e>text</span>;
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>contentEl</span>.<span style=color:#a6e22e>innerHTML</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>renderMarkdown</span>(<span style=color:#a6e22e>rawText</span>);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>When the user asks &ldquo;What day is it?&rdquo;, they see a brief &ldquo;Using tools: get_current_date, get_current_weekday&rdquo; indicator while the model calls the functions, followed by the streamed answer that includes the actual date. The whole round-trip - model decides to call a tool, PHP executes it, result goes back to the model, model generates the final answer - happens within a single SSE stream.</p><h2 id=what-we-built>What We Built<a hidden class=anchor aria-hidden=true href=#what-we-built>#</a></h2><p>Four phases, each building on the last:</p><ol><li><strong>JSON request/response</strong> - the simplest thing that works. A POST, a blocking <code>chat()</code> call, a JSON reply.</li><li><strong>SSE streaming</strong> - switch to <code>stream()</code> and <code>SSEResponse</code>, get real-time token delivery with connection-abort detection.</li><li><strong>Chat history</strong> - implement <code>ChatHistoryInterface</code> backed by a CodeIgniter model, give each conversation a thread ID, and the agent remembers everything.</li><li><strong>Tool usage</strong> - define callable tools in the agent, filter the stream for <code>ToolCallMessage</code> types, and let the LLM interact with your application.</li></ol><p>The progression shows a pattern: Neuron AI handles the AI complexity (prompt assembly, streaming protocol, tool orchestration), <code>SSEResponse</code> handles the HTTP complexity (headers, buffering, flushing), and your application code stays focused on business logic - which tools to expose, how to store history, what to show the user.</p><p>From here you could add RAG (retrieval-augmented generation) to let the agent search your documents, multi-agent workflows where agents delegate to each other, image generation, or any of the other capabilities Neuron AI supports. The foundation is the same: an agent, a provider, and a stream.</p><h2 id=source-code>Source Code<a hidden class=anchor aria-hidden=true href=#source-code>#</a></h2><p>The complete codebase for this chat application, including migrations, and frontend code, is available at <a href=https://github.com/michalsn/codeigniter-neuron-chat>github.com/michalsn/codeigniter-neuron-ai-chat</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://michalsn.dev/tags/codeigniter4/>Codeigniter4</a></li><li><a href=https://michalsn.dev/tags/php/>Php</a></li><li><a href=https://michalsn.dev/tags/ai/>Ai</a></li><li><a href=https://michalsn.dev/tags/ollama/>Ollama</a></li><li><a href=https://michalsn.dev/tags/neuron-ai/>Neuron-Ai</a></li><li><a href=https://michalsn.dev/tags/sse/>Sse</a></li><li><a href=https://michalsn.dev/tags/streaming/>Streaming</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on x" href="https://x.com/intent/tweet/?text=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI&amp;url=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f&amp;hashtags=codeigniter4%2cphp%2cai%2collama%2cneuron-ai%2csse%2cstreaming"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f&amp;title=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI&amp;summary=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI&amp;source=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f&title=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on whatsapp" href="https://api.whatsapp.com/send?text=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI%20-%20https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on telegram" href="https://telegram.me/share/url?text=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI&amp;url=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building an AI Chat with CodeIgniter, Ollama, and Neuron AI on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20an%20AI%20Chat%20with%20CodeIgniter%2c%20Ollama%2c%20and%20Neuron%20AI&u=https%3a%2f%2fmichalsn.dev%2fposts%2fbuilding-an-ai-chat-with-codeigniter-ollama-and-neuron-ai%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://utteranc.es/client.js repo=michalsn/michalsn.dev issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://michalsn.dev/>michalsn</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div id=cookie-notice role=dialog aria-live=polite aria-label="Cookie notice"><div class=inner><span>This website uses third-party cookies and scripts to improve its functionality.</span>
<a id=cookie-notice-accept class="btn btn-primary btn-sm">Approve</a>
<a id=cookie-notice-deny class="btn btn-primary btn-sm">Deny</a>
<a id=cookie-notice-info href=/privacy class="btn btn-primary btn-sm">More info</a></div></div><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>